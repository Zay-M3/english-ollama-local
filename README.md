# EnglisChat - Chat de Ingl√©s con IA Local

Esto es b√°sicamente una app de chat para practicar ingl√©s que usa Ollama + Mistral corriendo local en Docker. Nada de APIs externas, nada de tokens, nada de l√≠mites. Solo tu CPU trabajando.

![English Chat Interface](./assets/banner.png)

## ¬øQu√© hace?

- **Chat en tiempo real** con IA para practicar ingl√©s
- **Correcci√≥n autom√°tica** de gram√°tica y palabras  
- **Respuestas conversacionales** para mantener la charla
- **Todo local** - sin internet, sin APIs, sin costos
- **WebSockets** para chat fluido
- **Docker** para no volverse loco con dependencias

## Stack Tecnol√≥gico

### Frontend
- **React 19.1** + TypeScript + Vite
- **TailwindCSS** para estilos sin llorar
- **WebSockets** para chat en tiempo real
- **React Router** para navegaci√≥n

### Backend  
- **FastAPI** + Python 3.12
- **WebSockets** bidireccionales
- **httpx** para peticiones async
- **Pydantic** para validaci√≥n

### IA y Modelos
- **Ollama** corriendo en Docker
- **Mistral** (optimizado para CPU sin GPU)
- **Prompts optimizados** para respuestas r√°pidas (15 palabras max)

### Infraestructura
- **Docker Compose** para orquestar todo
- **n8n** (porque estaba experimentando)
- **Nginx** reverse proxy (pr√≥ximamente)

## Inicio R√°pido

```bash
# Clona esto
git clone https://github.com/tu-usuario/n8n-ollama-local.git
cd n8n-ollama-local

# Levanta todo (incluye descargar Mistral autom√°ticamente)
./setup.sh up mistral

# O si ya tienes modelos instalados
./setup.sh up

# Ver estado de los servicios
./setup.sh status

# Ver logs en tiempo real
./setup.sh logs ollama -f
```

**URLs despu√©s de levantar:**
- Chat: http://localhost:5173
- Backend API: http://localhost:3000
- n8n: http://localhost:5678
- Ollama: http://localhost:11434

## El Script setup.sh

Usa este script para manipular el proyecto de formas mas sencilla.

```bash
# Ver todos los comandos disponibles
./setup.sh

# Levantar servicios
./setup.sh up                    # Solo servicios
./setup.sh up llama3            # Servicios + instalar llama3

# Gesti√≥n de modelos
./setup.sh model install mistral
./setup.sh model list

# Control de servicios  
./setup.sh stop backend         # Parar solo backend
./setup.sh restart ollama       # Reiniciar ollama
./setup.sh status               # Ver estado general

# Limpieza
./setup.sh clean soft           # Bajar contenedores
./setup.sh clean hard           # Bajar + borrar vol√∫menes

# Logs y debugging
./setup.sh logs client -f       # Logs del frontend
./setup.sh logs backend         # Logs del backend
```

**Lo que hace internamente:**
1. Gestiona Docker Compose sin que tengas que recordar comandos
2. Espera a que Ollama est√© listo antes de instalar modelos
3. Descarga modelos autom√°ticamente si los especificas
4. Maneja errores de red y timeouts
5. Te da URLs para acceder a todo

## Optimizaciones para CPU

Este proyecto esta pensado para trabajar con un modelo peque√±o, para dispositivos con pocos recursos, aun asi existe la posibilidad de usar modelos de mayor
capacidad, recuerda que esto se corre directamente en tu CPU/GPU, asi que ten en cuenta esto al momento de probarlo.

### Configuraci√≥n Ollama
```yaml
# En docker-compose.yml
environment:
  - OLLAMA_CONTEXT_LENGTH=4096      # Contexto corto = m√°s r√°pido
  - OLLAMA_NUM_PARALLEL=1          # Una request a la vez
  - OLLAMA_KEEP_ALIVE=30m          # Mantener modelo en memoria
```

### Prompts Optimizados
```python
# Para correcciones (m√°x 12 palabras)
prompt = f'Correct any grammar or word mistakes in "{message}". If perfect, reply "Looks great!". Max 12 words.'

# Para respuestas (m√°x 15 palabras) 
prompt = f'Reply to "{message}" with a fun, creative answer (max 15 words). Ask a playful question.'
```

### Recursos Docker
```yaml
# 6GB RAM, 4 CPUs para Ollama
deploy:
  resources:
    limits:
      memory: 6G
      cpus: '4.0'
```

**Resultado:** Respuestas en 3-8 segundos en lugar de 15+ segundos.

## Estructura del Proyecto

```
 n8n-project/
‚îú‚îÄ‚îÄ  docker-compose.yml       # Orquestaci√≥n de servicios
‚îú‚îÄ‚îÄ  setup.sh                 # Script de gesti√≥n principal  
‚îú‚îÄ‚îÄ  app/
‚îÇ   ‚îú‚îÄ‚îÄ  client/              # React + TypeScript
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ  src/pages/chat/  # Componente principal de chat
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ  src/components/  # Componentes reutilizables
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ  src/utils/       # WebSocket hooks
‚îÇ   ‚îî‚îÄ‚îÄ  backend/             # FastAPI + Python
‚îÇ       ‚îú‚îÄ‚îÄ  api/v1/          # Endpoints REST y WebSocket
‚îÇ       ‚îú‚îÄ‚îÄ  services/        # L√≥gica de Ollama
‚îÇ       ‚îî‚îÄ‚îÄ  test/            # Tests unitarios
‚îî‚îÄ‚îÄ üìÅ n8n-data/               # Datos persistentes de n8n
```

## Funcionalidades

###  Chat Bidireccional
- **Env√≠as:** "I are good today"
- **IA corrige:** "Grammar error: 'am' not 'are'. Corrected: I am good today"  
- **IA responde:** "That's great! What's making your day so good?"

###  Optimizaciones
- **Sem√°foro:** Solo 1 request a Ollama simult√°nea
- **Timeouts inteligentes:** 8s, 11s, 14s con reintentos
- **Prompts cortos:** Menos tokens = m√°s velocidad
- **Context management:** 4096 tokens m√°ximo

## Disclaimer

Esto no es para producci√≥n. Es un experimento para:
-  Aprender sobre modelos locales
-  Practicar WebSockets bidireccionales  
-  Optimizar prompts y performance
-  Entender Docker networking
-  Probar arquitecturas de microservicios

Si te sirve para aprender algo, perfecto. Si encuentras bugs o mejoras, los PRs son bienvenidos.

No soy experto en ML/AI, solo quer√≠a que funcionara r√°pido en mi laptop sin GPU. Si ves algo que se puede optimizar mejor, av√≠same.

## Pr√≥ximas mejoras

- [ ] Nginx reverse proxy
- [ ] Autenticaci√≥n b√°sica
- [ ] M√°s modelos soportados
- [ ] M√©tricas de performance
- [ ] Deploy con Traefik
- [ ] Conversation history

---

**TL;DR:** Chat de ingl√©s con IA local usando React + FastAPI + Ollama en Docker. Sin APIs externas, sin costos, sin l√≠mites. Solo para aprender y practicar.
